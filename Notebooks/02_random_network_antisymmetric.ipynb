{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256272dd",
   "metadata": {},
   "source": [
    "# Simulation of randomly connected RNN with antisymmetric iSTDP\n",
    "\n",
    "This notebook simulates a random network of condunctance-based leaky integrate-and-fire (LIF) neurons under an antysimmetric covariance-based inhibitory spike-timing dependent plasticity (iSTDP) rule. The simuation is based on Brian2 (https://brian2.readthedocs.io).\n",
    "**This simulation replicates the results in Fig 3 G-K of the main paper**\n",
    "\n",
    "When using this code, please cite our work.\n",
    "\n",
    "> Festa, Dylan, Cusseddu, Claudia and Gjorgjieva, Julijana (2024) ‘Structured stabilization in recurrent neural circuits through inhibitory synaptic plasticity’. bioRxiv, p. 2024.10.12.618014. Available at: https://doi.org/10.1101/2024.10.12.618014.\n",
    "\n",
    "This notebook is intended as a demonstration. Athough it contains the network simulation in full, it does not show the full analysis of the output data and results may differ due to random initialization. See main README for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bf246",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "(if working locally, refer to *installation_instructions.md* to set up the local environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install brian2\n",
    "!pip install matplotlib\n",
    "# import packages\n",
    "import numpy as np\n",
    "from brian2 import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083507c7",
   "metadata": {},
   "source": [
    "Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c52fda",
   "metadata": {},
   "source": [
    "## Network and iSTDP parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE = 900         # Number of excitatory cells\n",
    "NI = 100          # Number of inhibitory cells\n",
    "tau_ampa = 5.0 # Glutamatergic synaptic time constant (ms)\n",
    "tau_gaba = 10.0 # GABAergic synaptic time constant (ms)\n",
    "# simulation time \n",
    "simtime_wup = 10.0 # Warmup time\n",
    "nsegments = 3\n",
    "simtime_spikerecorder = 30.0\n",
    "simtime_segment = 500.0\n",
    "# ###########################################\n",
    "# Neuron model\n",
    "# ###########################################\n",
    "gl = 10.0\n",
    "el = -60.0\n",
    "er = -80.0\n",
    "vt = -50.\n",
    "memc = 200.0  # Membrane capacitance\n",
    "# backgound currents\n",
    "bg_current_e = 0.0 # (pA)\n",
    "bg_current_i = 0.0 # (pA)\n",
    "# shared input for all population\n",
    "n_input_shared = 50\n",
    "rate_input_shared = 50.0 # (Hz)\n",
    "w_insh_e = 1.0 # (nS)\n",
    "w_insh_i = 0.0 # (nS)\n",
    "# independent exc input\n",
    "w_inindep_e = 1.0 # (nS)\n",
    "n_input_indep_e = 250 # \n",
    "rate_input_indep_e = 50.0 # (Hz)\n",
    "# independent inh input\n",
    "w_inindep_i = 0.1 # (mV)\n",
    "n_input_indep_i = 250\n",
    "rate_input_indep_i = 50.0 # (Hz)\n",
    "# Sparseness of synaptic connections\n",
    "connection_prob_ee = 0.2\n",
    "connection_prob_ei = 0.1\n",
    "connection_prob_ie = 1.0\n",
    "connection_prob_ii = 1.0 # denser is better\n",
    "# connection weights\n",
    "w_ee, w_ei, w_ie, w_ii = 1.0 , 1.0 , 1.8 , 0.3 # w_ie=3\n",
    "w_max = 80               # Maximum inhibitory weight\n",
    "# STDP parameters (antisymmetric, with some rate-regulation included)\n",
    "A0learn = 5E-3\n",
    "theta = -0.7\n",
    "gamma = 0.5\n",
    "alpha_pre = -0.7\n",
    "alpha_post = 0.2\n",
    "tauplus_stdp = 60.0    # STDP time constant (ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638c24b",
   "metadata": {},
   "source": [
    "## Network simulation code\n",
    "\n",
    "The block below runs the full network simulation in Brian2. Note that the iSTDP rule is defined by the equations in `eq_on_pre` and `eq_on_post`, corresponding to Eq 7 in the publication.\n",
    "\n",
    "**The simulation takes over 50 min on Google Colab, and 20 to 30 min on a personal computer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'NE is {NE} and NI is {NI}')\n",
    "\n",
    "eqs_neurons_e = '''\n",
    "dv/dt = (-(gl*nsiemens)*(v-el*mV)-(g_ampa*v+g_gaba*(v-er*mV))+bg_current_e*pA)/(memc*pfarad) : volt (unless refractory)\n",
    "dg_ampa/dt = -g_ampa/(tau_ampa*ms) : siemens\n",
    "dg_gaba/dt = -g_gaba/(tau_gaba*ms) : siemens\n",
    "'''\n",
    "\n",
    "eqs_neurons_i = '''\n",
    "dv/dt = (-(gl*nsiemens)*(v-el*mV)-(g_ampa*v+g_gaba*(v-er*mV))+bg_current_i*pA)/(memc*pfarad) : volt (unless refractory)\n",
    "dg_ampa/dt = -g_ampa/(tau_ampa*ms) : siemens\n",
    "dg_gaba/dt = -g_gaba/(tau_gaba*ms) : siemens\n",
    "'''\n",
    "\n",
    "# ###########################################\n",
    "# Initialize neuron group\n",
    "# ###########################################\n",
    "Pe = NeuronGroup(NE, model=eqs_neurons_e, threshold='v > vt*mV',\n",
    "reset='v=el*mV', refractory=5*ms, method='euler')\n",
    "\n",
    "Pi = NeuronGroup(NI, model=eqs_neurons_i, threshold='v > vt*mV',\n",
    "reset='v=el*mV', refractory=5*ms, method='euler')\n",
    "\n",
    "# shared input\n",
    "Pshared = PoissonGroup(n_input_shared, rates=rate_input_shared*Hz)\n",
    "con_shared_e = Synapses(Pshared, Pe, on_pre='g_ampa += w_insh_e*nS')\n",
    "con_shared_e.connect()\n",
    "con_shared_i = Synapses(Pshared, Pi, on_pre='g_ampa += w_insh_i*nS')\n",
    "con_shared_i.connect()\n",
    "\n",
    "# independent input\n",
    "PIe = PoissonInput(Pe, 'g_ampa', n_input_indep_e, rate_input_indep_e*Hz, weight=w_inindep_e*nS)\n",
    "PIi = PoissonInput(Pi, 'g_ampa', n_input_indep_i, rate_input_indep_i*Hz, weight=w_inindep_i*nS)\n",
    "\n",
    "# ##########################################\n",
    "# Connecting the network\n",
    "# ###########################################\n",
    "con_ee = Synapses(Pe, Pe, on_pre='g_ampa += w_ee*nS')\n",
    "con_ee.connect(condition='i!=j', p=connection_prob_ee)\n",
    "con_ei = Synapses(Pe, Pi, on_pre='g_ampa += w_ei*nS')\n",
    "con_ei.connect(p=connection_prob_ei)\n",
    "con_ii = Synapses(Pi, Pi, on_pre='g_gaba += w_ii*nS')\n",
    "con_ii.connect(condition='i!=j', p=connection_prob_ii)\n",
    "\n",
    "# ###########################################\n",
    "# Inhibitory Plasticity\n",
    "# ###########################################\n",
    "A0 = 0.0 # start with no learning\n",
    "\n",
    "# derived parameters\n",
    "tauminus_stdp = gamma*tauplus_stdp\n",
    "# NOT scaled by A0 here (since it controls learning on/off)\n",
    "Aplus = float(1/tauplus_stdp)*1E3\n",
    "Aminus = float(theta/tauminus_stdp)*1E3\n",
    "\n",
    "# simple traces for pre- and postsynaptic activity\n",
    "# (that need to be rescaled)\n",
    "\n",
    "eqs_stdp_inhib = '''\n",
    "    w : 1\n",
    "    dtrace_pre_plus/dt=-trace_pre_plus/(tauplus_stdp*ms) : 1 (event-driven)\n",
    "    dtrace_post_minus/dt=-trace_post_minus/(tauminus_stdp*ms) : 1 (event-driven)\n",
    "    '''\n",
    "eq_on_pre = '''\n",
    "    trace_pre_plus += 1.0\n",
    "    w = clip(w + A0*(alpha_pre + Aminus*trace_post_minus), 0, w_max)\n",
    "    g_gaba += w*nS\n",
    "    '''\n",
    "eq_on_post = '''\n",
    "    trace_post_minus += 1.0\n",
    "    w = clip(w + A0*(alpha_post + Aplus*trace_pre_plus), 0, w_max)\n",
    "    '''\n",
    "\n",
    "con_ie = Synapses(Pi, Pe, model=eqs_stdp_inhib,on_pre=eq_on_pre,on_post=eq_on_post)\n",
    "con_ie.connect()\n",
    "con_ie.w = w_ie\n",
    "\n",
    "# ###########################################\n",
    "# Setting up monitors\n",
    "# ###########################################\n",
    "wei_mon = StateMonitor(con_ie, 'w', record=True, dt=5.0*second)\n",
    "wei_mon.active = False\n",
    "pop_re_mon = PopulationRateMonitor(Pe)\n",
    "pop_re_mon.active = False\n",
    "pop_ri_mon = PopulationRateMonitor(Pi)\n",
    "pop_ri_mon.active = False\n",
    "\n",
    "sme = SpikeMonitor(Pe)\n",
    "smi = SpikeMonitor(Pi)\n",
    "sme.active = False\n",
    "smi.active = False\n",
    "\n",
    "# ###########################################\n",
    "# Warmup: no plasticity, low noise for all\n",
    "# ###########################################\n",
    "print('Running warmup')\n",
    "run(simtime_wup*second, report='text')\n",
    "\n",
    "# ###########################################\n",
    "# Record spikes right after warmup, plasticity still off\n",
    "# ###########################################\n",
    "print('Record after warmup, plasticity off')\n",
    "sme.active = True\n",
    "smi.active = True\n",
    "pop_re_mon.active = True\n",
    "pop_ri_mon.active = True\n",
    "wei_mon.active = True \n",
    "run(simtime_spikerecorder*second, report='text')\n",
    "\n",
    "# ###########################################\n",
    "# For loop on segments\n",
    "# ###########################################\n",
    "# plasticity on\n",
    "A0 = A0learn\n",
    "for thesegment in range(nsegments):\n",
    "    print('Now running segment ', thesegment + 1, ' of ', nsegments, '\\n')\n",
    "    # recorder off\n",
    "    sme.active = False\n",
    "    smi.active = False\n",
    "    run(simtime_segment*second, report='text')\n",
    "    # recorder on\n",
    "    sme.active = True\n",
    "    smi.active = True\n",
    "    run(simtime_spikerecorder*second, report='text')\n",
    "\n",
    "print('******* \\n All runs completed!\\n*******')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be0b4a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "### Population rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract population rates and time from monitors\n",
    "pop_re_times = pop_re_mon.t / second\n",
    "pop_re_rates = pop_re_mon.smooth_rate(window='flat',width=0.5*second) / Hz\n",
    "\n",
    "pop_ri_times = pop_ri_mon.t / second\n",
    "pop_ri_rates = pop_ri_mon.smooth_rate(window='flat',width=0.5*second) / Hz\n",
    "\n",
    "# Create the plot\n",
    "nplot = 200\n",
    "idxplot = np.linspace(start=1,stop=len(pop_re_times)-1,num=nplot).round().astype(int)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pop_re_times[idxplot]/60, pop_re_rates[idxplot], label='Excitatory',color='blue')\n",
    "plt.plot(pop_ri_times[idxplot]/60, pop_ri_rates[idxplot], label='Inhibitory',color='red')\n",
    "plt.xlabel('time (min)')\n",
    "plt.ylabel('population Rate (Hz)')\n",
    "plt.title('Population Rates over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee8086-75b8-45da-89e9-bcfb4b94087c",
   "metadata": {},
   "source": [
    "This plot shows that, before plasticity starts, population rates are near saturation, therefore the system is unstable. Inhibitory plasticity reduces the firing rates to much lower levels. The circuit is therefore inhibition-stabilized. The very sharp drop in rates here is due to the parameter choice in the inhibitory plasticity rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462559e",
   "metadata": {},
   "source": [
    "### Distribution of final mutual vs unidirectional weights\n",
    "\n",
    "(this needs some improvement in the code, but the results is as intended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b355bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists for mutual and unidirectional weights\n",
    "w_ie_mutual = []\n",
    "w_ie_unidirectional = []\n",
    "# count number of connections too\n",
    "n_mutual = 0\n",
    "n_uni = 0\n",
    "# build exc to inh weight matrix in full\n",
    "w_ei_dense = np.zeros((NE,NI))\n",
    "w_ei_dense[con_ei.i,con_ei.j]=w_ei\n",
    "\n",
    "# now iterate over inh to exc connections\n",
    "n_con_ie = len(con_ie.w)\n",
    "for idx_ie_i, idx_ie_j,k in zip(con_ie.i, con_ie.j,range(n_con_ie)):\n",
    "    # Check if a reciprocal connection exists in con_ei\n",
    "    is_mutual =  w_ei_dense[idx_ie_j,idx_ie_i] > 1E-4\n",
    "    this_w = con_ie.w[k]        \n",
    "    if is_mutual:\n",
    "        n_mutual+=1\n",
    "        w_ie_mutual.append(this_w)\n",
    "    else:\n",
    "        n_uni+=1\n",
    "        w_ie_unidirectional.append(this_w)\n",
    "\n",
    "# print total mutual and total unidirectional\n",
    "print(f\"found {n_mutual} mutual connections and {n_uni} unidirectional connections. In total {n_mutual+n_uni}\")\n",
    "        \n",
    "# Convert lists to numpy arrays\n",
    "w_ie_mutual = np.array(w_ie_mutual)\n",
    "w_ie_unidirectional = np.array(w_ie_unidirectional)\n",
    "# Calculate histogram values and normalize\n",
    "hist_mutual, bins_mutual = np.histogram(w_ie_mutual, bins=np.arange(0,25, 1), density=True)\n",
    "hist_unidirectional, bins_unidirectional = np.histogram(w_ie_unidirectional, bins=np.arange(0, 25, 1), density=True)\n",
    "\n",
    "# Replace zero values with a small positive value to avoid log(0) error\n",
    "hist_mutual[hist_mutual == 0] = 1e-6 \n",
    "hist_unidirectional[hist_unidirectional == 0] = 1e-6\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "# Normal scale plot\n",
    "ax1.hist(bins_mutual[:-1], bins_mutual, weights=hist_mutual, color='brown', alpha=0.7, label='Mutual')\n",
    "ax1.hist(bins_unidirectional[:-1], bins_unidirectional, weights=hist_unidirectional, color='darkcyan', alpha=0.7, label='Unidirectional')\n",
    "ax1.set_xlabel('Synaptic Weight')\n",
    "ax1.set_ylabel('Normalized Frequency')\n",
    "ax1.set_title('Distribution of Weights (Normal Scale)')\n",
    "ax1.legend()\n",
    "\n",
    "# Logarithmic scale plot\n",
    "ax2.hist(bins_mutual[:-1], bins_mutual, weights=hist_mutual, color='brown', alpha=0.7, label='Mutual')\n",
    "ax2.hist(bins_unidirectional[:-1], bins_unidirectional, weights=hist_unidirectional, color='darkcyan', alpha=0.7, label='Unidirectional')\n",
    "ax2.set_yscale('log')  # Set logarithmic y-axis\n",
    "ax2.set_xlabel('Synaptic Weight')\n",
    "ax2.set_ylabel('density (log scale)')\n",
    "ax2.set_title('Distribution of Weights (Logarithmic Scale)')\n",
    "ax2.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b8454-702f-4d19-aa59-5dd2db0fb00c",
   "metadata": {},
   "source": [
    "When examining the inh to exc weights after learning, one can clearly see a net separation between mutual connections and unidirectional connections. The choice of antisymmetric, covariance-dominated inhibitory plasticity results in much stronger unidirectional connections than mutual connections."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
